\chapter{Optimizing performance}

\begin{abstract}
  In this chapter we explain how to make your photobiology calculations execute as fast as possible. The code has been profiled and the performance bottlenecks removed in most cases the implementing some functions in C++. Furthermore copying of spectra is minimized by using package \code{data.table} as the base class of all objects where spectral data is stored. However, it is possible to improve performance even more by changing some defaults and writing efficient user code. This is what is discussed in the present chapter, and should not be of concern unless several thousands of spectra need to be processed.
\end{abstract}

\section{Packages used in this chapter}

For executing the examples listed in this chapter you need first to load the following packages from the library:
<<>>=
library(photobiology)
library(photobiologyVIS)
library(photobiologyUV)
library(microbenchmark)
@

Although not a recommended practice, just to keep the examples shorter, we \code{attach} a data set for the solar spectrum:
<<>>=
attach(sun.data)
@

%%%%
\section{Introduction}\label{sec:perf:intro}

When developing the current version of \PB quite a lot of effort was spent in optimizing performance, as in one of our experiments, we need to process several hundreds of thousands of measured spectra. The defaults should provide good performance in most cases, however, some further improvements are achievable, when a series of different calculations are done on the same spectrum, or when a series of spectra measured at exactly the same wavelengths are used for calculating weighted irradiances or exposures.

There is also a lot you can achieve by carefully writing the code in your own scripts. The packages themselves are fairly well optimized for speed. In your own code try to avoid unnecessary copying of big objects. The \textsf{r4photobiology} suite makes extensive use of the \code{data.table} package, using it also in your own code could help. Try to avoid use of explicit loops by replacing them with vectorized operations, and when sequentially building vectors in a loop, preallocate an object big enough before entering the loop.

Being R an interpreted language, there is rather little automatic code optimization taking place, so you may find that even simple things like moving invariant calculations out of loops, and avoiding repeated calculations of the same value by storing the value in a variable can improve performance.

This type of `good style' optimizations have been done throughout the suite's code, and more specific problem identified by profiling and and dealt with case by case. Of course, to achieve maximum overall performance, to should follow the same approach with your own code.

%%%%
\section{Task: avoiding repeated validation}\label{sec:perf:check}

In the case of doing calculations repeatedly on the same spectrum, a small improvement in performance can be achieved by setting the parameter \code{check.spectrum=FALSE} for all but the first call to \code{irradiance()}, or \code{photon\_irradiance()}, or \code{energy\_irradiance()}, or the equivalent functions for ratios. It is also possible to set this parameter to FALSE in all calls, and do the check beforehand by explicitly calling \code{check\_spectrum()}.

%%%%
\section{Task: caching of multipliers}\label{sec:perf:caching}

In the case of calculating weighted irradiances on many spectra having exactly the same wavelength values, then a significant improvement in the performance can be achieved by setting \code{use.cached.mult=TRUE}, as this reuses the multipliers calculated during successive calls based on the same waveband. However, to achieve this increase in performance, the tests to ensure that the wavelength values have not changed, have to be kept to the minimum. Currently only the length of the wavelength array is checked, and the cached values discarded and recalculated if the length changes. For this reason, this is not the default, and when using caching the user is responsible for making sure that the array of wavelengths has not changed between calls.

%%%%
\section{Task: benchmarking}\label{sec:perf:benchmark}

You can use the package \code{microbenchmark} to time the code and find the parts that slow it down. I have used it, and also
I have used profiling to optimize the code for speed. The examples below show how choosing different values from the defaults
can speed up calculations when the same calculations are done repeatedly on spectra measured at exactly the same wavelengths, something which is usual when analyzing spectra measured with the same instrument. The choice of defaults is based on what is
best when processing a moderate number of spectra, say less than a few hundreds, as opposed to many thousands.

<<>>=
library(microbenchmark)
@

\subsection{Convenience functions}

The convenience functions are slightly slower than the generic \code{irradiance} function.

<<>>=
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  irradiance(w.length, s.e.irrad, PAR(), unit.out="photon",
             use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
@

Using the generic reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\% if using the cache.

\subsection{Using cached multipliers}

Using the cache when repeatedly applying the same waveband has a large impact on the execution time.

<<>>=
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, PAR()),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
@
When using an unweighted waveband the cache reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.

When using BSWFs the speed up by use of the cache is more important, and dependent on the complexity of the equation used in the calculation.

<<>>=
res1 <- microbenchmark(
  energy_irradiance(w.length, s.e.irrad, CIE()),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  energy_irradiance(w.length, s.e.irrad, CIE(), use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
@

When using a weighted waveband, in this example, \code{CIE()}, the cache reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.

\subsection{Disabling checks}

Disabling the checking of the spectrum halves once again the execution time for unweighted wavebands.

<<>>=
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE,
                    check.spectrum=FALSE),
  times=100L, control=list(warmup = 10L))
@
When using an unweighted waveband, in this example, \code{PAR()}, the disabling the data validation checking reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.

\subsection{Using stored wavebands}

Saving a waveband object and reusing it, can give an additional speed up when all other optimizations are also used.

<<>>=
myPAR <- PAR()
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE,
                    check.spectrum=FALSE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, myPAR, use.cache=TRUE,
                    check.spectrum=FALSE),
  times=100L, control=list(warmup = 10L))
@
When using an unweighted waveband, in this example, \code{PAR()}, using a saved waveband object reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.

Saving a waveband object that uses weighting and reusing it, gives an additional speed up when all other optimizations are also used.

<<>>=
myCIE <- CIE()
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, CIE(), use.cache=TRUE,
                    check.spectrum=FALSE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, myCIE, use.cache=TRUE,
                    check.spectrum=FALSE),
  times=100L, control=list(warmup = 10L))
@
When using a weighted waveband, in this example, \code{CIE()}, using a saved waveband object reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.



\subsection{Inserting hinges}

Inserting `hinges' to reduce integration errors slows down the computations considerably. If the spectral data is measured with a small wavelength step, the errors are rather small. By default the use of `hinges' is automatically decided based on the average wavelength step in the spectral data. The `cost' of using hinges depends on the waveband definition, as BSWFs with discontinuities in the slope require several hinges, while unweighted one requires at most two, one at each boundary.

<<>>=
res1 <- microbenchmark(
  energy_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  energy_irradiance(w.length, s.e.irrad, PAR(), use.cache=TRUE,
                    use.hinges=TRUE),
  times=100L, control=list(warmup = 10L))
@
When using an uweighted waveband, in this example, \code{PAR()}, enabling use of hinges increases the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by a  factor of \Sexpr{signif(med2 / med1)}.

Inserting `hinges' to reduce integration errors slows down the computations a lot. If the spectral data is measured with a
small wavelength step, the errors are rather small. By default the use of `hinges' is automatically decided based on the
average wavelength step in the spectral data.

<<>>=
res1 <- microbenchmark(
  energy_irradiance(w.length, s.e.irrad, CIE(), use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  energy_irradiance(w.length, s.e.irrad, CIE(), use.cache=TRUE,
                    use.hinges=TRUE),
  times=100L, control=list(warmup = 10L))
@
When using an weighted waveband, in this example, \code{CIE()}, enabling use of hinges increases the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by a  factor of \Sexpr{signif(med2 / med1)}.

\section{Overall speed-up achievable}

\subsection{\code{GEN.G}}

If we consider a slow computation, using a BSWF with a complex equation like \code{GEN.G}, we can check the best case improvement in throughput that can be ---on a given hardware and software system.

<<tidy=FALSE>>=
# slowest
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, GEN.G(),
                    use.cache=FALSE,
                    use.hinges=TRUE,
                    check.spectrum=TRUE),
                      times=100L, control=list(warmup = 10L))
# default
res2 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, GEN.G()),
                       times=100L, control=list(warmup = 10L))

# fastest
gen.g <- GEN.G()
res3 <- microbenchmark(
  irradiance(w.length, s.e.irrad, gen.g,
             use.cache=TRUE,
             use.hinges=FALSE,
             check.spectrum=FALSE,
             unit.out="photon"),
                       times=100L, control=list(warmup = 10L))
@
When using a weighted waveband, in this example, \code{GEN.G()}, enabling all checks and optimizations for precision, and disabling all optimizations for speed yields a median execution time of \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms, accepting all defaults yields a median execution time \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, and disabling all checks, optimizations for precision and enabling all optimizations for speed yields a median execution time of \Sexpr{med3 <- median(res3$time) * 1e-6; signif(med3, 3)}, in relation to the slowest one, execution times are \Sexpr{signif(med1 / max(c(med1, med2, med3)) * 1e2, 2)}, \Sexpr{signif(med2 / max(c(med1, med2, med3)) * 1e2, 2)}, and \Sexpr{signif(med3 / max(c(med1, med2, med3)) * 1e2, 2)}\%.

Finally we compare the returned values for the irradiance, to see the impact on them of optimizing for speed.

<<tidy=FALSE>>=
# slowest
photon_irradiance(w.length, s.e.irrad, GEN.G(),
                  use.cache=FALSE,
                  use.hinges=TRUE,
                  check.spectrum=TRUE)

# default
photon_irradiance(w.length, s.e.irrad, GEN.G())

# fastest
gen.g <- GEN.G()
irradiance(w.length, s.e.irrad, gen.g,
           use.cache=TRUE,
           use.hinges=FALSE,
           check.spectrum=FALSE,
           unit.out="photon")
@

These results are based on spectral data at 1 nm interval, for more densely measured data the effect of not using hinges becomes even smaller. In contrast, with data measured at wider wavelength steps, the errors will be larger. They also depend on the specific BSWF being used.

\subsection{\code{CIE}}

If we consider a slow computation, using a BSWF with a complex equation like \code{CIE}, we can check the best case improvement in throughput that can be ---on a given hardware and software system.

<<tidy=FALSE>>=
# slowest
res1 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, CIE(),
                    use.cache=FALSE,
                    use.hinges=TRUE,
                    check.spectrum=TRUE),
                      times=100L, control=list(warmup = 10L))
# default
res2 <- microbenchmark(
  photon_irradiance(w.length, s.e.irrad, CIE()),
                       times=100L, control=list(warmup = 10L))

# fastest
cie <- CIE()
res3 <- microbenchmark(
  irradiance(w.length, s.e.irrad, cie,
             use.cache=TRUE,
             use.hinges=FALSE,
             check.spectrum=FALSE,
             unit.out="photon"),
                       times=100L, control=list(warmup = 10L))
@
When using a weighted waveband, in this example, \code{CIE()}, enabling all checks and optimizations for precision, and disabling all optimizations for speed yields a median execution time of \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms, accepting all defaults yields a median execution time \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, and disabling all checks, optimizations for precision and enabling all optimizations for speed yields a median execution time of \Sexpr{med3 <- median(res3$time) * 1e-6; signif(med3, 3)}, in relation to the slowest one, execution times are \Sexpr{signif(med1 / max(c(med1, med2, med3)) * 1e2, 2)}, \Sexpr{signif(med2 / max(c(med1, med2, med3)) * 1e2, 2)}, and \Sexpr{signif(med3 / max(c(med1, med2, med3)) * 1e2, 2)}\%.

Finally we compare the returned values for the irradiance, to see the impact on them of optimizing for speed.

<<tidy=FALSE>>=
# slowest
photon_irradiance(w.length, s.e.irrad, CIE(),
                  use.cache=FALSE,
                  use.hinges=TRUE,
                  check.spectrum=TRUE)

# default
photon_irradiance(w.length, s.e.irrad, CIE())

# fastest
CIE <- CIE()
irradiance(w.length, s.e.irrad, CIE,
           use.cache=TRUE,
           use.hinges=FALSE,
           check.spectrum=FALSE,
           unit.out="photon")
@

These results are based on spectral data at 1 nm interval, for more densely measured data the effect of not using hinges becomes even smaller. In contrast, with data measured at wider wavelength steps, the errors will be larger. They also depend on the specific BSWF being used.

\subsection{Using \code{split\_irradiance}}

Using the cache also helps with \code{split\_irradiance}.

<<>>=
res1 <- microbenchmark(
  split_photon_irradiance(w.length, s.e.irrad,
                          c(400, 500, 600, 700)),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  split_photon_irradiance(w.length, s.e.irrad,
                          c(400, 500, 600, 700),
                          use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
@
When using \code{split\_irradiance}, the cache reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.

Using hinges slows down calculations:

<<>>=
res1 <- microbenchmark(
  split_photon_irradiance(w.length, s.e.irrad,
                          c(400, 500, 600, 700),
                          use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  split_photon_irradiance(w.length, s.e.irrad,
                          c(400, 500, 600, 700),
                          use.cache=TRUE,
                          use.hinges=TRUE),
  times=100L, control=list(warmup = 10L))
@
When using \code{split\_irradiance}, enabling use of hinges increases the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms,  by a factor of \Sexpr{signif(med2 / med1)}. There is less overhead than if calculating the same three wavebands separately, as all hinges are inserted in a single operation.

Disabling checking of the spectrum reduces the execution time, but proportionally not as much as for the \code{irradiance} functions, as the spectrum is checked only once independently of the number of bands into which it is split.

<<>>=
res1 <- microbenchmark(
  split_photon_irradiance(w.length, s.e.irrad,
                          c(400, 500, 600, 700),
                          use.cache=TRUE),
  times=100L, control=list(warmup = 10L))
res2 <- microbenchmark(
  split_photon_irradiance(w.length, s.e.irrad,
                          c(400, 500, 600, 700),
                          use.cache=TRUE,
                          check.spectrum=FALSE),
  times=100L, control=list(warmup = 10L))
@
When using \code{split\_irradiance}, disabling the data validation check reduces the median execution time from \Sexpr{med1 <- median(res1$time) * 1e-6; signif(med1, 3)} ms to  \Sexpr{med2 <- median(res2$time) * 1e-6; signif(med2, 3)} ms, by \Sexpr{signif((med1 - med2) / med1 * 100, 2)}\%.

As all the execution times are in milliseconds, all the optimizations discussed above are totally irrelevant unless you are planning to repeat similar calculations on thousands of spectra. They apply only to the machine, OS and version of R and packages used when building this typeset output.

%%%%
\section{Profiling}\label{sec:perf:profiling}

Profiling is basically fine-grained benchmarking. It provides information about in which part of your code the program spends most time when executing. Once you know this, you can try to just make those critical sections execute faster. Speed-ups can be obtained either by rewriting these parts in a compiled language like C or C++, or by use of a more efficient calculation algorithm. A detailed discussion is outside the scope of this handbook, so only a brief example will be shown here.

<<>>=
detach(sun.data)
@

